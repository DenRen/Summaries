7\.  Параллельные вычисления
========================

## 7.1 Предпочитайте программирование на основе *задач* программированию на основе *потоков*.

Есть два варианта запустить *doAsyncWork* асинхронно:
```cpp
int doAsyncWork ();

std::thread (doAsyncWork);    // Поток

auto fut = std::async (doAsyncWork); // fut = future
```
Преимущества *std::async*:
* Даёт возвращающее значение в вызывающий код
* Есть доступ к исключению, которое бросила функция (в потоках будет *std::terminate*)
* Решает проблему превышения подписки

Бывают ситуации, когда применение *потоков* является более подходящим:
* Вам нужен доступ к *API*, лежащий в основе реализации потоков (*pthreads* или *Windows Threads*)
* Вам требуется возможность оптимизации потоков в вашем приложении
* Вам требуется реализовать поточную технологию, выходящую за рамки *API* параллельных вычислений *C++*

### <center>Следует запомнить</center>
* API *std::thread* не предлагает способа непосредственного получения возвращаемых значений из асинхронно выполняемых функций, и, если такие функции генерируют исключения, программа завершается.
* Программирование на основе потоков требует управления вручную исчерпанием потоков, превышением подписки, балансом загрузки и адаптацией к новым платформам.
* Программирование на основе задач с помощью *std::async* со стратегией запуска по умолчанию решает большинство перечисленных проблем вместо вас.

## 7.2 Если важна асинхронность, указывайте std::launch::async

У *std::async* есть стратегии запуска:
* **Стратегия** *std::launch:async* означает, что *func* должна выполнятся асинхронно, т.е. в другом потоке
* **Стратегия** *std::launch::deferref* означает, что *func* может выполняться только тогда, когда для *фьючерса*, возвращённого *std::async*, вызывается функция-член *get* или *wait*, т.е. выполнение *func* *откладывается* до тех пор, пока не будет выполнен такой вызов. Когда вызываются функции-члены *get* или *wait*, функция *func* выполняется синхронно, т.е. вызывающая функция блокируется до тех пор, пока *func* не завершит работу. Если не вызывается ни *get*, ни *wait*, *func* не выполняется.

Два приведённых вызова имеют один и тот же смысл:
```cpp
auto fut1 = std::async (func);                   // Выполнение func со
                                                    // стратегиейпо умолчанию
                                                 
auto fut2 = std::async (std::launch::async |     // Выполнение func
                         std::launch::deferred,   // асинхронное и отложенное
                         func);
```

У стратегии запуска по умолчанию есть интересные последствия. Для потока *t*, выполняющего приведённую ниже инструкцию, справедливы следующие утверждения:
```cpp
auto fut = std::async (func); // Выполнение func со стратегией запуска по умолчанию
```
* **Невозможно предсказать, будет ли *func* выполняться параллельно с *t***, поскольку выполнение *func* может быть отложено планировщиком.
* **Невозможно предсказать, будет ли *func* выполняться потоком, отличным о того, в котором вызываются функции-члены *get* или *wait* объекта *fut***. Если этот поток - *t*, отсюда вытекает невозможность предсказать, будет ли *func* выполняться потоком, отличным от *t*.
* **Может быть невозможно предсказать, будет ли *func* выполнена вообще**, поскольку может оказаться невозможность гарантировать, что функции-члены *get* или *wait* объекта *fut* будут вызваны на всех путях выполнения программы.

*Локальная память потока* (*thread-local-storage* - *TLS*)

Гибкость стратегии запуска по умолчанию часто плохо комбинируется с использованием переменных *thread_local*.

Также при использовании стратегии запуска по умолчанию мы можем зайти в бесконечный цикл даже не подозревая этого:
```cpp
using namespace std::literals;

void func () {
    std::this_thread::sleep_for (1s);
}

auto future = std::async (func);    // (Концептуально) асинхронное
                                    // выполнение func

while (future.wait_for (100ms) !=   // Цикл до завершения func
       std::future_status::ready)
{
    // ...                          // ... которого никодга не будет!
}

```

Чтобы избегать подобных ситуаций, нужно проверять фьючерс и выяснять, не отложенная ли данная задача:
```cpp
auto future = std::async (func);    // (Концептуально) асинхронное
                                    // выполнение func
if (future.wait_for (0s) ==
    std::future_status::deferred)
{
    // Используем get или wait для синхронного
    // вызова func
} else {
    while (future.wait_for (100ms) !=
       std::future_status::ready)
    {
        // ...                          // func выполняется параллельно
    }
    
    // future выполнен
}
```

Получается *std::async* со стратегией запуска по умолчанию отлично работает, пока выполняются следующие условия:
* Задача не обязана параллельно работать с потоком, вызывающим *get* или *wait*
* Не имеет значения, переменные *thread_local* какого потока читаются или записываются
* Либо гарантируется вызова *get* или *wait* для фьючерса, возвращаемого *std::async*, либо ситуация, когда задача не выполняется совсем, является примелемой
* Код с использованием *wait_for* или *wait_until* учитывает возможность отложенного состояния задачи

Иногда полезно иметь функцию, которая запускает функцию с *std::launch::async*:
```cpp
template <typename F, typename... Ts>
inline
std::future <std::result_of_t <F (Ts...)>>     // С++14 auto
reallyAsync (F&& f, Ts&&... params)
{
    return std::async (
        std::launch::async,
        std::forward <F> (f),
        std::forward <Ts> (params)...
    );
}
```

### <center>Следует запомнить</center>
* Стратегия запуска по умолчанию для *std::async* допускает как асинхронное, так и синхронное выполнение задачи.
* Эта гибкость ведёт к неопределённости при обращении к переменным *thread_local*, к тому, что задача может никогда не быть выполнена, и влияет на логику программы для вызовов *wait* на основе тайм-аутов.
* Если синхронное выполнение задачи критично, указывайте стратегию запуска *std::launch::async*.

## 7.3 Делайте *std::thread* неподключаемым на всех путях выполнения

Каждый объект *std::thread* может находиться в двух состояниях: *подключаемом* (*joinable*) и *неподключаемом* (*unjoinable*). Подключаемый может или уже выполняется.

Неподключаемые объекты *std::thread*:
* Объекты *std::thread*, созданные конструктором по умолчанию
* Объекты *std::thread*, из которых выполнено перемещение
* Объекты *std::thread*, из которых выполнена функция-член *join*
* Объекты *std::thread*, из которых выполнена функция-член *detach*

В С++14 можно разделять большие числа:
```cpp
constexpr auto tenMillion = 10`000`000;
```

```cpp
constexpr auto tenMillion = 10`000`000;

bool doWork (std::function <bool (int)> filter,
             int maxVal = tenMillion)
{
    std::vector <int> goodVals;
    std::thread thread (
        [&filter, maxVal, &goodVal]
        {
            for (auto i = 0; i <= maxVal; ++i) {
                if (filter (i))
                    goodVals.push_back (i);
            }
        });
        
    auto nh = thread.native_handle ();
    
    if (conditionsAreSatisfied ()) {
        thread.join ();
        performComputation (goodVals);
        return true;
    }
    
    return false;    // Ошибка!!! Завершение программы!!!
}
```

При вызове деструктора из подключаемого объекта программа завершает свою работу, поэтому с *std::thread* нужно быть аккуратным.

*RAII* (*Resource Acquisition Is Initialization*) - *захват ресурса есть инициализация* (На самом деле речь идёт о методе деструкции). Примеры: контейнеры STL, стандартные интеллектуальные указатели, объекты *std::fstream*. Но нет стандартного *RAII* класса для *std::thread*. Напишем его сами:
```cpp
class ThreadRAII {
public:
    enum class DtorAction { join, detach };
    
    ThreadRAII (std::thread&& thread, // Т.к. std::thread не копируется
                DtorAction action) :
        thread (thread),
        action (action)
    {}
    
    ~ThreadRAII () {
        if (thread.joinable ()) {
            if (action == DtorAction::join)
                thread.join ();
            else
                thread.detach ();
        }
    }
    
    ThreadRAII (ThreadRAII&&) = default;            // Поддержка перемещающих
    ThreadRAII& operator= (ThreadRAII&&) = default; // операций
    
    std::thread& get () {    // Чтобы не дублировать интерфейс std::thread
        return thread;
    }
    
private:
    DtorAction action;
    std::thread thread; // Объявлять потоки в конце хорошая привычка
};
```

Тогда наш предыдущий код можно записать следующим образом:
```cpp
bool doWork (std::function <bool (int)> filter,
             int maxVal = tenMillion)
{
    std::vector <int> goodVals;
    
    ThreadRAII thread (
        std::thread (
            [&filter, maxVal, &goodVal]
            {
                for (auto i = 0; i <= maxVal; ++i) {
                    if (filter (i))
                        goodVals.push_back (i);
                }
            }),
        ThreadRAII::DtorAction::join    // Действие RAII
    );
        
    auto nh = thread.get().native_handle ();
    
    if (conditionsAreSatisfied ()) {
        thread.get().join ();
        performComputation (goodVals);
        return true;
    }
    
    return false;    // Ошибка!!! Завершение программы!!!
}
```

Но что, если *join* будет приводить к зависанию программы? Чтобы справиться с этим, нужны *прерываемые потоки*. Увы, С++11 их е поддерживает.

### <center>Следует запомнить</center>
* Делайте *std::thread* неподключаемым на всех путях выполнения.
* Применение *join* при уничтожении объекта может привести к трудно отлаживаемым аномалиям производительности.
* Применение *detach* при уничтожении объекта может привести к трудноотлавливаемому неопределённому поведению.
* Объявляйте объекты *std::thread* в списке членов-данных последними.

## 7.4 Помните о разном поведении деструкторов дескрипторов потоков

Объекты *std::thread* и объекты фьючерсов можно рассматривать как *дескрипторы* (*handles*) системных потоков.
```cpp
// | Вызывающая |     фьючерс    std::promise (обычно) | Вызываемая |
// | функци     | <----------------------------------- | функция    |
```

Где хранится результат вызываемой функции, если мы используем *std::async*? Он не может храниться ни в объектах вызываемой функции, ни в объектах вызывающей функции. Он должен хранится где-то вне их. Это место называется *общее состояние* (*shared state*).
```cpp
//                               Общее состояние
// | Вызывающая |     фьючерс     | Результат |  std::promise (обычно) | Вызываемая |
// |   функци   | <-------------- |  вызова   | <--------------------- | функция    |
```

Для поведения деструктора фьючерса справедливо следующее:
* Деструктор последнего фьючерса, ссылающегося на общее состояние для не отложенной задачи, запущенной с помощью *std::async*, блокируется до тех пор, пока задача не будет завершена.
* Деструкторы всех прочих фьючерсов просто уничтожают объект фьючерса.

Нормальное поведение - это когда деструктор фьючерса уничтожает объект фьючерса. Исключение из этого нормального поведения для фьючерса выполняются когда верны следующие условия:
* Он ссылается на общее состояние, созданное вызовом *std::async*
* Стратегия запуска задачи - *std::launch::async*
* Фьючерс  является последним фьючерсом, ссылающимся на общее состояние. Это всегда справедливо для *std::future*, для *std::shared_future* только при одном объекте *std::share_future*.

Когда все эти условия выполнены, деструктор фьючерса демонстрирует особое поведение: состоит в блокировке, пока не будет завершена асинхронно выполняющаяся задача (по сути неявный *join*). Говорят: "Фьючерс из *std::async* блокируется в своём деструкторе".

Как следствие можно иногда не знать, будет ли у вас блокироваться деструктор, потому что сам объект *std::future* не знает этого. Разве что из логики программы:
```cpp
std::vector <std::future <void>> vect; // Может заблокироваться в деструкторе

class Widget {
public:
    // ..
private:
    std::shared_future <double> fut;   // Может заблокироваться в деструкторе
};
```

Если работать с *std::packaged_task*, то никакого особого поведения не будет.

### <center></center>
* Деструкторы фьючерсов обычно просто уничтожают данные-члены фьючерсов.
* Последний фьючерс, ссылающийся на общее состояние неотложенной задачи, запущенной с помощью *std::async*, блокируется до завершения этой задачи.

## 7.5 Применяйте фьючерсы *void* для одноразовых сообщений о событиях

Для сигнализации о событии можно использовать *переменную условия*. *Задача обнаружения*. *Задача реакции*.
При
```cpp
std::condition_variable condvar;
std::mutex mutex;
```
код задачи обнаружения очень прост:
```cpp
// ...            // Обнаружение события
cv.notify_one (); // Уведомление задачи реакции
```

Код задачи реакции:
```cpp
// ...
{
    std::unique_lock <std::mutex> lock (m);
    cv.wait ();
    // ...
}
```

Правда этот *код с душком* (*code smell*). Даже если команда работает, что-то выглядит не совсем верным. Запах от мьютексов. Может от не нужен?

Даже если опустит вопрос с мьютексом, то остаются две проблемы:
* Если задача обнаружение уведомляет переменную условия до вызова *wait* задачи реакции, задача реакции "зависнет".
* Вызов *wait* приводит к ложным пробуждениям. (*Ложное пробуждение* (*Spurious wakeup*))

Некоторые программисты для решения похожих проблем синхронизации используют булев флаг, но это очень неэффективно! Чистой воды *busy wait*.

Вот так можно реализовать корректный код. Задача обнаружения:
```cpp
std::condition_variable condvar;
std::mutex mutex;
bool flag  (false);

// ...
{
    std::lock_guard <std::mutex> guard (mutex);      // Установка блокировки
    flag = true;
}                                                    // Снятие блокировки

cv.notify_one ();

// ...
```

Задача реакции:
```cpp
// ...
{
    std::unique_lock <std::mutex> lock (mutex);
    condvar.wait (lock, [] { return flag; });
    
    // Реакция на событие
}
// ...
```

Лямбда нужна от ложных пробуждений.

Этот код работает корректно, но от него всё равно остался запах мьютекса. Поэтому можно предложить другой вариант. Что нам нужно? Взаимодействовать с несколькими потоками. А мы же уже делали это, когда использовали *std::async*. Поэтому давайте использовать объекты, с которыми он работает: *std::promise* и *std::future*!

Тогда задача обнаружения понимает, что произошло событие, она *устанавливает* *std::promise* (выполняет запись в канал связи), а задача реакции выполняет вызов *wait*.

Тогда:
```cpp
std::promise <void> prom; // Коммуникационный канал
```

Задача обнаружения:
```cpp
// ...
prom.set_value ();
```

Задача реакции:
```cpp
// ...
prom.get_future ().wait ();
// ...
```

 Разве не идеально? Не совсем: используется динамическая память и вообще это является *одноразовым* механизмом.

Можно специально приостанавливать поток, например, чтобы его настроить до начала выполнения:
```cpp
std::promise <void> prom;

void react ();
void detect () {
    std::thread thread (
        [] {
            prom.get_future ().wait ();
            react ();
        }
    );
    // Настраиваем thread
    
    prom.set_value ();
    
    // ...

    prom.join (); // Делаем thread неподключаемым
}
```

Аналогично можно сделать со множеством потоков:
```cpp
std::promise <void> prom;

void detect () {  
    auto sp = prom.get_future ().share ();  // sp - shared_future <void>
    
    std::vector <std::thread> threads;
    
    for (int i = 0; i < threadsToRun; ++i) {
        threads.emplace_back (
            [sf] {                // По значению
                sf.wait ();
                react ();
            }
        );
    }
    
    // ...
    
    prom.set_value ();  // Продолжаем все потоки
    
    // ...
    
    for (auto& thread : threads)
        thread.join ();    // Присоединяем потоки
}
```

Эти коды элегантны, но есть одно большая проблема, которую здесь мы не будем решать: проблема вылета исключения после создания потока(ов) и перед *prom.set_value ()*. Если оно вылетит, то подключаемые потоки вызовут деструктор, т.е. мы получим *std::terminate*!

### <center>Следует запомнить</center>
* Для простого сообщения о событии дизайн с применением переменных условия требует избыточных мьютексов, накладывает ограничения на относительное выполнение задач обнаружения и реакции и требует от задачи реакции проверки того, что событие в действительности имело место.
* Дизайн с использованием флага устраняет эти проблемы, но использует опрос, а не блокировку.
* Переменные условия и флаги могут быть использованы совместно, но получающийся механизм сообщения оказывается несколько неестественным.
* Применение объектов *std::promise* и фьючерсов решает указанные проблемы, но этот подход использует динамическую память для общих состояний и ограничен одноразовым сообщением.

## 7.6 Используйте *std::atomic* для параллельности, *volatile* - для особой памяти

Все следующие операции *атомарны*:
```cpp
std::atomic <int> ai (0);
ai = 10;
std::cout << ai;  // Атомарно чтение ai, а не std::cout
++ai;
--ai;
```

*RMW* - *Read-Modify-Write*

Все функции-члены *std::atomic*, включая *RMW*-операции, будут рассматриваться другими потоками как атомарные.

Такой же код, но при использовании *volatile* в многопоточном контексте не гарантирует почти ничего:
```cpp
volatile int vi (0);
vi = 10;
std::cout << vi;
++vi;
--vi;
```

Компиляторы могут спокойно переупорядочивать операции, если они видят, что переменные независимы в данном потоке:
```cpp
a = b;
x = y;
```
Компилятор может (а если не сделает, то может и аппаратура или покажется ядрам) переместить их местами:
```cpp
x = y;
a = b;
```

Это может быть очень критичным, если мы вычисляем значение, а потом выставляем флаг готовности, ведь компилятор может поменять их местами:
```cpp
std::atomic <bool> valAvailable (false);

auto impValue = computeImportantValue ();
valAvailable = true;
```
 Но у *std::atomic*есть важное свойство: никакой предшествующий код в исходном тексте записи переменной *std::atomic* не может иметь место (или выглядеть таковым для других ядер) после неё. Т.е. они не только гарантируют последовательность кода, но ещё и гарантируют, что так же себя поведёт и аппаратное обеспечение.

Т.е. предыдущий код выполнится строго последовательно.

У *std::atomic* есть две модели согласованности: *последовательная* и *слабая* (*смягчённая*).

Применение *volatile* не накладывает такие ограничения компилятору. *volatile* нужен для *особой* памяти.

У *std::atomic* нет копирующих операций, т.е. нельзя создать *std::atomic* из *std::atomic* копированием, но есть методы *load* и *store*, которые позволяют читать и записывать значения *std::atomic*:
```cpp
std::atomic <int> x (19);
std::atomic <int> y (x.load ()); // Конструирование не атомарно!
y.store (x.load ());
```

В итоге становится очевидным:
* *std::atomic* применяется в параллельном программировании, но не для доступа к особой памяти
* *volatile* применяется для доступа к особой памяти, но не в параллельном программировании

Так как они служат разным целям, то их можно использовать совместно:
```cpp
volatile std::atomic <int> vai; // Операции над vai атомарны и не могут быть
                                 // удалены при оптимизации
```

Это полезно для ячейки отображаемой памяти с параллельным доступом.

Примечание: некоторые разработчики предпочитают использовать *load* и *store* для *std::atomic* даже там, где это не требуется, просто чтобы указать, что данная переменная является *std::atomic*. Это полезно, чтобы видеть потенциально узкие места масштабируемости.

### <center>Следует запомнить</center>
* *std::atomic* применяется для обращения нескольких потоков к данным без использования мьютексов. Это инструмент параллельного программирования.
* *volatile* применяется для памяти, чтение и запись которой не должны удаляться при оптимизации. Это инструмент для работы с особой памятью.